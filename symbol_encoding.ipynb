{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symbol encoding\n",
    "\n",
    "### How it works?\n",
    "\n",
    "* We can compress a sequence of symbols if each one is translated by a code-word and,\n",
    "  in average, the lengths of the code-words are smaller than the\n",
    "  length of the symbols.\n",
    "  \n",
    "* The encoder and the decoder have a probabilistic model $M$ which\n",
    "  provides to a variable-length encoder ($C$)/decoder($C^{-1}$) the\n",
    "  probability $p(s)$ of each symbol $s$.\n",
    "  \n",
    "* The most probable symbols are represented by the shorter\n",
    "  code-words and viceversa.\n",
    "\n",
    "<img src=\"data/compresion_entropica.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bits, data and information\n",
    "\n",
    "* data != information (data is the representation of the information).\n",
    "\n",
    "* Lossless data compression uses a shorter representation for\n",
    "  information.\n",
    "  \n",
    "* By definition, a bit of data stores a bit of information, if and\n",
    "  only if, it represents the occurrence of an equiprobable event (an\n",
    "  event that can be true or false with the same probability).\n",
    "  In this ideal situation, the representation is fully efficient\n",
    "  (no futher compression would be possible).\n",
    "  \n",
    "* By definition, a symbol $s$ with probability $p(s)$ stores\n",
    "  \\begin{equation}\n",
    "    I(s)=-\\log_2 p(s) \\tag{Eq:symbol_information}\n",
    "  \\end{equation}\n",
    "  bits of information.\n",
    "\n",
    "  <img src=\"data/prob_vs_long.png\" width=\"600\">\n",
    "\n",
    "* So, ideally, the length of a code-word in bits (of data) should match with the number bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "* The entropy $H(S)$ measures the amount of information per\n",
    "  symbol that a source of information $S$ produces, in average. By definition\n",
    "  \\begin{equation}\n",
    "    H(S) = \\frac{1}{N}\\sum_{s=1}^{N} p(s)\\times I(s)\n",
    "  \\end{equation}\n",
    "  bits-of-information/symbol, where $N$ is the size of the source\n",
    "  alphabet (number of different symbols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compression basics\n",
    "\n",
    "### Encoding of a symbol\n",
    "\n",
    "1. While the decoder does not know the symbol:\n",
    "    1. Assert something about the symbol that allows to the decoder\n",
    "    to minimize the uncertainty of finding that symbol. This guess\n",
    "    should have true or false with the same probability.\n",
    "    2. Output a bit of code that says if the last guess is true or\n",
    "    false.\n",
    "    \n",
    "### Decoding of a symbol\n",
    "\n",
    "1. While the symbol is not known without uncertainty:\n",
    "    1. Make the same guess that the encoder.\n",
    "    2. Input a bit of code that represents the result of the last\n",
    "    guess.\n",
    "    \n",
    "#### Tip\n",
    "\n",
    "* This codec is 100% efficient if the guesses are equiprobable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Let's suppose that we use the Spanish alphabet. Humans know that\n",
    "  symbols does not form words in any order, so we can\n",
    "  formulate the following VLC (Variable Length Codec):\n",
    "  \n",
    "  In Spanish there are 28 letters. Therefore, to encode, for example,\n",
    "  the word `preciosa`, the first symbol `p` can be represented by\n",
    "  its index inside of the Spahish alphabet with a code-word of 5 bits. In\n",
    "  this try, the encoding is not a very efficient, but this we are in first\n",
    "  letter ... For the second one `r` we can see (using a\n",
    "  Spanish dictionary) that after a `p`, the following symbols are\n",
    "  possible: (1) `a`, (2) `e`, (3) `i`, (4) `l`, (5) `n`, (6)\n",
    "  `o`, (7) `r`, (8) `s` and (9) `u`. Therefore, we don't need\n",
    "  5 bits now, 4 are enough.\n",
    "  \n",
    "<img src=\"data/universal_coding_example.png\" width=\"600\">\n",
    "\n",
    "* Notice that the compression ratio has been 40/25:1 (`preciosa` has 8\n",
    "  letters)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
